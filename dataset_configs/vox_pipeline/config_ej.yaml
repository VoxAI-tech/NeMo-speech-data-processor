documentation: |
  Vox Pipeline - El Jannah Australia Drive-thru Audio Dataset Creation (Qwen LLM Version)
  ========================================================================================
  
  Overview
  --------
  This configuration processes drive-thru audio conversations from El Jannah Australia.
  Uses Qwen LLM for menu correction instead of Gemini audio verification (faster).
  Handles English language audio with customer (mic) and staff (spk) channels.
  
  Data Structure:
  - Input: data/audio/ej/{device_id}/{year}/{month}/{day}/{hour}/{session_id}/{mic.wav, spk.wav}
  - Each session has two WAV files: mic (customer) and spk (employee)
  - 4 devices: 90104C41, 1200UD26, 1840UD05, 1853UD05
  
  Pipeline Stages:
  1. Create initial manifest from drive-thru audio directory
    3. Language detection and filtering (English)
  2. Two-pass Whisper transcription (segments + slice-by-offset)
  3. Hallucination detection and filtering
  4. Menu fuzzy matching (first pass correction)
  5. Qwen LLM menu correction (spelling and menu items)
  6. Qwen punctuation/capitalization restoration
  7. Text normalization with regex
  8. Optional conversion to tarred dataset
  9. Output final manifest with all metadata
  
  Usage:
    python main.py \
      --config-path dataset_configs/vox_pipeline/ \
      --config-name config_ej.yaml \
      data_dir=data/audio \
      output_dir=outputs/vox_pipeline_ej \
      sdp_dir=/path/to/NeMo-speech-data-processor

# Base configuration paths
data_dir: ???  # Required: Path to raw audio data
output_dir: ???  # Required: Path where outputs will be saved
sdp_dir: ???  # Required: Path to NeMo-speech-data-processor repo
cache_dir: ${output_dir}/cache

# Brand and country configuration
brand: ej  # El Jannah
country: au  # Australia

# Processing parameters
max_samples: null  # Set to limit samples for testing
params:
  audio_channel: mic  # 'mic' for customer or 'spk' for employee
  max_samples: -1  # -1 for all samples
  source_lang: en  # English
  source_lang_full: English
  min_audio_lid_probability: 0.7
  min_audio_duration: 0.5  # seconds
  max_audio_duration: 30  # seconds
  use_regex: common  # Options: common, all
  convert_to_audio_tarred_dataset:
    should_run: false
    num_shards: 1
    buckets_num: 1
  save_disk_space: false  # If true, removes intermediate files

# Which processors to run
processors_to_run: all

# Use Dask for parallel processing
use_dask: false  # Set to false to avoid import issues

# Pipeline processors
processors:
  # ========== STAGE 0: Create initial manifest from Vox directory structure ==========
  - _target_: sdp.processors.datasets.vox.create_initial_manifest.CreateInitialManifestVox
    raw_data_dir: ${data_dir}
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_00.json
    brand_code: ej  # El Jannah
    country_code: au  # Australia
    brand_name: El Jannah
    country_name: Australia
    speaker: ${params.audio_channel}  # mic or spk

  # ========== STAGE 1: Create unique audio ID ==========
  - _target_: sdp.processors.LambdaExpression
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_00.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_01.json
    new_field: audio_id
    expression: 'entry.device_id + "/" + entry.session_id + "/" + ("mic" if entry.audio_type == "customer" else "spk")'

  # ========== STAGE 2: Limit samples for testing (optional) ==========
  - _target_: sdp.processors.modify_manifest.limit_samples.LimitManifestEntries
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_01.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    max_entries: ${max_samples}


  # ========== STAGE 3: Get audio duration ==========
  - _target_: sdp.processors.GetAudioDuration
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    audio_filepath_key: audio_filepath
    duration_key: duration

  # ========== STAGE 4: Language detection with Whisper ==========
  - _target_: sdp.processors.FasterWhisperInference
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    model_size_or_path: large-v3
    num_devices: -1  # Use all available GPUs
    output_dir: ${output_dir}/${params.source_lang}/step_05
    language_detection_only: true
    inference:
      language_detection_segments: 7
      chunk_length: 30
    save_timestamps_separately: false
    skip_corrupted_audios: true

  # ========== STAGE 5: Filter by language ==========
  - _target_: sdp.processors.LambdaExpression
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_03.json
    new_field: lid_verified
    expression: '(entry.language == "${params.source_lang}") & (entry.language_probability >= ${params.min_audio_lid_probability})'
    filter: true

  # ========== STAGE 6: Drop language detection fields ==========
  - _target_: sdp.processors.DropSpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_03.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_04.json
    fields_to_drop:
      - language
      - language_probability
      - lid_verified

  # ========== STAGE 7: Full transcription with segments ==========
  - _target_: sdp.processors.FasterWhisperInference
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_04.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_05.json
    model_size_or_path: large-v3  # Using standard large-v3 for English
    output_dir: ${output_dir}/${params.source_lang}/step_08
    num_devices: -1
    inference:
      language: ${params.source_lang}
    save_timestamps_separately: false
    skip_corrupted_audios: true

  # ========== STAGE 8: Drop duration (will be recalculated per segment) ==========
  - _target_: sdp.processors.DropSpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_05.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_06.json
    fields_to_drop:
      - duration

  # ========== STAGE 9: Filter out empty segments before ListToEntries ==========
  - _target_: sdp.processors.LambdaExpression
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_06.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_07.json
    new_field: has_segments
    expression: 'len(entry.segments) > 0'
    filter: true

  # ========== STAGE 10: Convert segments list to individual entries ==========
  - _target_: sdp.processors.ListToEntries
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_07.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_08.json
    field_with_list: segments

  # ========== STAGE 11: Keep only needed segment fields ==========
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_08.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_09.json
    fields_to_keep:
      - audio_filepath
      - id
      - start
      - end
      - text
      - language
      - session_id
      - device_id
      - audio_type
      - audio_id

  # ========== STAGE 12: Calculate segment duration ==========
  - _target_: sdp.processors.LambdaExpression
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_09.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_10.json
    new_field: duration
    expression: 'entry.end - entry.start'

  # ========== STAGE 13: Filter by duration ==========
  - _target_: sdp.processors.DropHighLowDuration
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_10.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_11.json
    high_duration_threshold: ${params.max_audio_duration}
    low_duration_threshold: ${params.min_audio_duration}

  # ========== STAGE 14: Rename fields for segments ==========
  - _target_: sdp.processors.RenameFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_11.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_12.json
    rename_fields:
      start: offset
      id: segment_id
      language: source_lang

  # ========== STAGE 15: Keep core fields ==========
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_12.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_13.json
    fields_to_keep:
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration
      - session_id
      - device_id
      - audio_type
      - audio_id

  # ========== STAGE 16: Transcription by slice (more accurate for segments) ==========
  - _target_: sdp.processors.FasterWhisperInference
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_13.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_14.json
    model_size_or_path: large-v3  # Using standard large-v3 for English
    num_devices: -1
    output_dir: ${output_dir}/${params.source_lang}/step_16
    inference:
      language: ${params.source_lang}
    save_timestamps_separately: false
    skip_corrupted_audios: true
    slice_by_offset: true  # Use offset and duration to slice audio

  # ========== STAGE 17: Keep transcription results ==========
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_14.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_15.json
    fields_to_keep:
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration
      - pred_text
      - session_id
      - device_id
      - audio_type
      - audio_id

  # ========== STAGE 18: Rename pred_text to text ==========
  - _target_: sdp.processors.RenameFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_15.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_16.json
    rename_fields:
      pred_text: text

  # ========== STAGE 19: Drop empty transcriptions ==========
  - _target_: sdp.processors.DropIfRegexMatch
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_16.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_17.json
    text_key: text
    regex_patterns:
      - '^\s*$'  # Empty or whitespace-only

  # ========== STAGE 20: Detect Whisper hallucinations ==========
  - _target_: sdp.processors.DetectWhisperHallucinationFeatures
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_17.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_18.json
    common_hall_file: ${sdp_dir}/dataset_configs/vox_pipeline/partials/common_phrases/${params.source_lang}.txt
    text_field: text

  # ========== STAGE 21: Filter hallucinated segments ==========
  - _target_: sdp.processors.LambdaExpression
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_18.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_19.json
    new_field: is_hallucinated
    expression: '(not entry.hall_repeated_ngrams) & (not entry.hall_long_word) & (not entry.hall_frequent_single_word)'
    filter: true

  # ========== STAGE 22: Clean hallucination fields ==========
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_19.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_20.json
    fields_to_keep:
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration
      - text
      - audio_type
      - device_id
      - session_id
      - audio_id
      
  # ========== STAGE 23: Menu-Aware Correction (Fuzzy Matching - First Pass) ==========
  - _target_: sdp.processors.menu_aware_correction.MenuAwareCorrection
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_20.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_21.json
    menu_vocabulary_file: ${sdp_dir}/dataset_configs/vox_pipeline/assets/vocabularies/menu_vocabulary.json  # El Jannah vocabulary
    text_field: text
    corrected_field: text_menu_corrected
    fuzzy_threshold: 80
    context_window: 3
    save_corrections: true

  # ========== STAGE 24: Qwen LLM Menu Correction (Instead of Gemini) ==========
  # Uses Qwen to correct menu item spellings based on context
  - _target_: sdp.processors.inference.llm.vllm.vllm.vLLMInference
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_21.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_22.json
    prompt_file: ${sdp_dir}/dataset_configs/vox_pipeline/partials/menu_correction_prompts/${params.source_lang}.yaml
    generation_field: llm_corrected_text
    model:
      model: Qwen/Qwen3-8B
      tensor_parallel_size: 1
      max_model_len: 2048
      enable_chunked_prefill: true
      max_num_batched_tokens: 1024
      enforce_eager: true
      dtype: float16
      gpu_memory_utilization: 0.9
      max_num_seqs: 2
    inference:
      temperature: 0.1
      top_p: 0.95
      top_k: 50
      repetition_penalty: 1.0
      max_tokens: 2048
    apply_chat_template:
      tokenize: false
      add_generation_prompt: true
      enable_thinking: false
    save_outputs: true
    output_dir: ${output_dir}/${params.source_lang}/llm_menu_corrections
    error_log_file: ${output_dir}/${params.source_lang}/qwen_llm_errors.json

  # ========== STAGE 25: Punctuation & Capitalization Restoration with Qwen ==========
  - _target_: sdp.processors.inference.llm.vllm.vllm.vLLMInference
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_22.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_23.json
    generation_field: src_text
    prompt_file: ${sdp_dir}/dataset_configs/vox_pipeline/partials/pr_recovery_prompts/${params.source_lang}.yaml
    model:
      model: Qwen/Qwen3-8B
      tensor_parallel_size: 1
      max_model_len: 2048
      enable_chunked_prefill: true
      max_num_batched_tokens: 512
      enforce_eager: true
      dtype: float16
      gpu_memory_utilization: 0.9
      max_num_seqs: 4
    inference:
      temperature: 0.3
      top_p: 0.8
      top_k: 20
      repetition_penalty: 1.05
      max_tokens: 2048
    apply_chat_template:
      tokenize: false
      add_generation_prompt: true
      enable_thinking: false

  # ========== STAGE 26: Clean Qwen Generation Output ==========
  - _target_: sdp.processors.CleanQwenGeneration
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_23.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_24.json
    text_field: llm_corrected_text  # Original text to keep
    generation_field: src_text  # Qwen output to clean

  # ========== STAGE 27: Text Normalization with Regex ==========
  - _target_: sdp.processors.SubRegex
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_24.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_25.json
    text_key: src_text  # Apply to Qwen-processed text
    regex_params_yaml: ${sdp_dir}/dataset_configs/vox_pipeline/partials/subregex_params/${params.use_regex}.yaml

  # ========== STAGE 28: Drop intermediate text fields ==========
  - _target_: sdp.processors.DropSpecifiedFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_25.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_26.json
    fields_to_drop:
      - text
      - text_menu_corrected
      - menu_corrections
      - num_corrections
      - gemini_corrections
      - llm_corrected_text

  # ========== STAGE 29: Rename final text field ==========
  - _target_: sdp.processors.RenameFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_26.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_27.json
    rename_fields:
      src_text: text  # Final cleaned text
      offset: source_audio_offset
      
  # ========== STAGE 30: Add constant metadata fields ==========
  - _target_: sdp.processors.AddConstantFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_27.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_28.json
    fields:
      dataset: ${brand}_${country}_drive_thru
      language: ${params.source_lang}
    
  # ========== STAGE 31: Final field selection ==========
  - _target_: sdp.processors.flexible_field_keeper.FlexibleKeepFields
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_28.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_29.json
    fields_to_keep:
      - audio_filepath
      - text
      - duration
      - source_audio_offset
      - segment_id
      - audio_type
      - device_id
      - session_id
      - audio_id
      - dataset
      - language
      - gemini_confidence
      - gemini_correction_made

  # ========== STAGE 32: Convert to WebDataset (TAR archives with wav/json pairs) ==========
  - _target_: sdp.processors.convert_to_webdataset.ConvertToWebDataset
    input_manifest_file: ${output_dir}/${params.source_lang}/manifest_29.json
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_30.json
    output_dir: ${output_dir}/${params.source_lang}/webdataset
    shard_size: 100  # Files per shard
    split: train
    prefix: shard
    shuffle: true
    shuffle_seed: 42
    slice_with_offset: true  # Extract audio segments
    speaker_field: speaker  # Include in metadata
    include_metadata: true